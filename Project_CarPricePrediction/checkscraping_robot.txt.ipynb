{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fb61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No robots.txt file found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def check_web_scraping_allowed(url):\n",
    "    # Construct the URL for the robots.txt file\n",
    "    robots_url = url + \"/robots.txt\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to fetch the robots.txt file\n",
    "        response = requests.get(robots_url)\n",
    "\n",
    "        # Check if the response is successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Check if web scraping is allowed or disallowed\n",
    "            if \"User-agent: *\" in response.text:\n",
    "                if \"Disallow: /\" in response.text:\n",
    "                    return \"Web scraping is disallowed for this website.\"\n",
    "                else:\n",
    "                    return \"Web scraping is allowed for this website.\"\n",
    "\n",
    "        # Return a message if the robots.txt file is not found or inaccessible\n",
    "        return \"No robots.txt file found.\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"An error occurred while checking the robots.txt file.\"\n",
    "\n",
    "# Call the function with the website URL\n",
    "website_url = \"https://www.enterprise.com/en/home.html\"\n",
    "result = check_web_scraping_allowed(website_url)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48023611",
   "metadata": {},
   "source": [
    "\n",
    "When the response is \"No robots.txt file found,\" it typically indicates that the website does not have a specific robots.txt file that provides instructions for web crawlers and scrapers. In such cases, it is generally assumed that web scraping is allowed, but it's still essential to review the website's terms of service or consult with the website owner to ensure scraping is permitted.\n",
    "\n",
    "Keep in mind that scraping websites without permission may still be subject to legal and ethical considerations. It is always recommended to act responsibly, respect the website's policies, and comply with applicable laws and regulations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
